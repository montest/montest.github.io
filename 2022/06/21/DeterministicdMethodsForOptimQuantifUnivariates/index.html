

<!doctype html>
<html lang="en" class="no-js">
  <head>
    




<meta charset="utf-8">



<!-- begin SEO -->









<title>Deterministic Numerical Methods for Optimal Voronoï Quantization: The one-dimensional case - Thibaut Montes</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Thibaut Montes">
<meta property="og:title" content="Deterministic Numerical Methods for Optimal Voronoï Quantization: The one-dimensional case">


  <link rel="canonical" href="https://montest.github.io/2022/06/21/DeterministicdMethodsForOptimQuantifUnivariates/">
  <meta property="og:url" content="https://montest.github.io/2022/06/21/DeterministicdMethodsForOptimQuantifUnivariates/">



  <meta property="og:description" content=" In my previous blog post, I detailed the methdos used to build an optimal Voronoï quantizer of random vectors \(X\) whatever the dimension \(d\). In this post, I will focus on real valued random variables and present faster methods for dimension $1$. All the code presented in this blog post is available in the following Github repository: montest/deterministic-methods-optimal-quantization.">





  

  





  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2022-06-21T00:00:00+02:00">








  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Thibaut Montes",
      "url" : "https://montest.github.io",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="https://montest.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Thibaut Montes Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://montest.github.io/assets/css/main.css">

<meta http-equiv="cleartype" content="on">

    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="https://montest.github.io/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="https://montest.github.io/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="https://montest.github.io/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="https://montest.github.io/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="https://montest.github.io/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="https://montest.github.io/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="https://montest.github.io/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="https://montest.github.io/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="https://montest.github.io/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="https://montest.github.io/images/optim2d_bokeh.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="https://montest.github.io/images/optim2d_bokeh.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="https://montest.github.io/images/optim2d_bokeh.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="https://montest.github.io/images/optim2d_bokeh.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="https://montest.github.io/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="https://montest.github.io/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/optim2d_bokeh.png?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="https://montest.github.io/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="https://montest.github.io/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="https://montest.github.io/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://montest.github.io/">Thibaut Montes</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://montest.github.io/resume/">Resume</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://montest.github.io/year-archive/">Blog Posts</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://montest.github.io/publications/">Publications</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://montest.github.io/talks/">Talks</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://montest.github.io/research/">Research</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="https://montest.github.io/images/me.jpg" class="author__avatar" alt="Thibaut Montes, PhD">
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Thibaut Montes, PhD</h3>
    <p class="author__bio">Machine Learning Data Scientist</p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Talkwalker, Luxembourg</li>
      
      
      
      
      
       
      
      
      
      
        <li><a href="https://www.linkedin.com/in/thibaut-montes-194a77a9"><i class="fab fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
      
      
      
      
      
      
        <li><a href="https://github.com/montest"><i class="fab fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.fr/citations?user=o8jICwcAAAAJ&hl=en"><i class="fas fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Deterministic Numerical Methods for Optimal Voronoï Quantization: The one-dimensional case">
    <meta itemprop="description" content=" In my previous blog post, I detailed the methdos used to build an optimal Voronoï quantizer of random vectors \(X\) whatever the dimension \(d\). In this post, I will focus on real valued random variables and present faster methods for dimension $1$. All the code presented in this blog post is available in the following Github repository: montest/deterministic-methods-optimal-quantization.">
    <meta itemprop="datePublished" content="June 21, 2022">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Deterministic Numerical Methods for Optimal Voronoï Quantization: The one-dimensional case
</h1>
          
            <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  23 minute read
	
</p>
          
        

        
          <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2022-06-21T00:00:00+02:00">June 21, 2022</time></p>
        


        

        </header>
      

      <section class="page__content" itemprop="text">
        <h1 class="no_toc" id="table-of-contents">Table of contents</h1>

<ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#what-is-so-special-about-the-1-dimensional-case" id="markdown-toc-what-is-so-special-about-the-1-dimensional-case">What is so special about the 1-dimensional case?</a>    <ul>
      <li><a href="#vertices" id="markdown-toc-vertices">Vertices</a></li>
      <li><a href="#closed-form-formulas" id="markdown-toc-closed-form-formulas">Closed-form formulas</a></li>
    </ul>
  </li>
  <li><a href="#going-back-to-optimization-methods" id="markdown-toc-going-back-to-optimization-methods">Going back to optimization methods</a>    <ul>
      <li><a href="#fixed-point-search-deterministic-lloyd-method" id="markdown-toc-fixed-point-search-deterministic-lloyd-method">Fixed-point search: Deterministic Lloyd method</a></li>
      <li><a href="#gradient-descent" id="markdown-toc-gradient-descent">Gradient descent</a>        <ul>
          <li><a href="#mean-field-clvq" id="markdown-toc-mean-field-clvq">Mean-field CLVQ</a></li>
          <li><a href="#newton-raphson-method" id="markdown-toc-newton-raphson-method">Newton-Raphson method</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#numerical-examples" id="markdown-toc-numerical-examples">Numerical examples</a>    <ul>
      <li><a href="#normal-distribution" id="markdown-toc-normal-distribution">Normal distribution</a></li>
      <li><a href="#log-normal-distribution" id="markdown-toc-log-normal-distribution">Log-Normal distribution</a></li>
      <li><a href="#exponential-distribution" id="markdown-toc-exponential-distribution">Exponential distribution</a></li>
    </ul>
  </li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>

<h1 id="introduction">Introduction</h1>

<p>In my <a href="/2022/02/13/StochasticMethodsForOptimQuantif/">previous blog post</a>, I detailed the two main Monte-Carlo simulation-based procedures used to build an optimal Voronoï quantizer of random vectors \(X\) whatever the dimension \(d\).</p>

<p>In this post, I will focus on real valued random variables and explain how to efficiently build optimal quantizers in dimension 1. All the code presented in this blog post is available in the following Github repository: <a href="https://github.com/montest/deterministic-methods-optimal-quantization">montest/deterministic-methods-optimal-quantization</a>. The main idea is to create an abstract class <code class="language-plaintext highlighter-rouge">VoronoiQuantization1D</code> that will contains all generic methods that can be used in order to optimize an optimal quantizer as well as some useful methods in order to compute the distortion, its gradient and hessian. And then implement the methods specific to the distribution of $X$ in the derived classes (e.g <code class="language-plaintext highlighter-rouge">NormalVoronoiQuantization</code>, <code class="language-plaintext highlighter-rouge">UniformVoronoiQuantization</code>, <code class="language-plaintext highlighter-rouge">LogNormalVoronoiQuantization</code>, <code class="language-plaintext highlighter-rouge">ExponentialVoronoiQuantization</code>). I will need the following packages:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">scipy</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Union</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">field</span>
</code></pre></div></div>

<h1 id="what-is-so-special-about-the-1-dimensional-case">What is so special about the 1-dimensional case?</h1>

<p>In the following figure, I display in red the optimal quantizer of a standard normal distribution of size $N=11$ and the vertices of the cell $C_i(\Gamma_N)$ are represented by black lines on the real axis. The probability associated to the quantizer $x_i^N$ is the integral on the cell $C_i(\Gamma_N)$ of the normal density, as represented in the figure.</p>

<center>
    <img alt="VoronoiQuantizationGaussian1D" src="/images/posts/quantization/univariate/vor_quantif_gaussian_10_cut.png" width="700" />
</center>

<p>We can notice that the Voronoï cells associated to a quantizer $\widehat X^N$ are intervals in $\mathbb{R}$. Moreover, we can easily compute the coordinate of each vertice (the border of each interval).</p>

<h2 id="vertices">Vertices</h2>

<p>If we consider that the centroids $(x_i^N)_i$ are ordered: \(x_1^N &lt; x_2^N &lt; \cdots &lt; x_{N-1}^N &lt; x_{N}^N\), then the Voronoï cells are given by</p>

\[C_i ( \Gamma_N ) =
    \left\{ \begin{aligned}
        &amp; \big( x_{i - 1/2}^N , x_{i + 1/2}^N \big] &amp;\qquad i = 1, \dots, N-1 \\
        &amp; \big( x_{i - 1/2}^N , x_{i + 1/2}^N \big) &amp; i = N
    \end{aligned} \right.\]

<p>where the vertices $x_{i-1/2}^N$ are defined by</p>

\[\forall i = 2, \dots, N, \qquad x_{i-1/2}^N := \frac{x_{i-1}^N + x_i^N}{2}\]

<p>and</p>

\[x_{1/2}^N := \textrm{inf} (\textrm{supp} (\mathbb{P}_{_{X}})) \, \textrm{ and } \, x_{N+1/2}^N := \textrm{sup} (\textrm{supp} (\mathbb{P}_{_{X}})).\]

<p>In practice, for a list of <strong>sorted</strong> (from lower to bigger) <code class="language-plaintext highlighter-rouge">centroids</code> of size $N$, the <code class="language-plaintext highlighter-rouge">vertices</code> can be found using the following method that returns a list of size $N+1$ containing the following: the lower-bound and upper-bound of the support of $X$ and $N-1$ points that are the mid-points between each centroid. For example, for a standard normal distribution, the lower-bound and upper-bound of the support are $- \infty$ and $+ \infty$, respectively. The method <code class="language-plaintext highlighter-rouge">get_vertices</code> is defined in the abstract class <code class="language-plaintext highlighter-rouge">VoronoiQuantization1D</code> because this method in independent of the random distribution of $X$.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">VoronoiQuantization1D</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>

    <span class="n">lower_bound_support</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">upper_bound_support</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_vertices</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">centroids</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="n">vertices</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">centroids</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="n">centroids</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">vertices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">vertices</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">lower_bound_support</span><span class="p">)</span>
        <span class="n">vertices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">vertices</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">upper_bound_support</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">vertices</span>
</code></pre></div></div>

<h2 id="closed-form-formulas">Closed-form formulas</h2>

<p>Let $X$ is a random variable taking values in \(\mathbb{R}\). I remind the expression of the quadratic distortion we wish to minimize in order to build an optimal quantizer $\widehat X^N$. Given an $N$-tuple $x := (x_1^N, \dots, x_N^N) $, the distortion is given by</p>

<!-- $$
    \begin{aligned}
	   \mathcal{Q}_{2,N} : x \longmapsto \frac{1}{2} \mathbb{E} \Big[ \min_{i = 1, \dots, N} \vert X - x_i^N \vert^2 \Big] &= \frac{1}{2} \Vert X - \widehat X^N \Vert_{_2}^2 \\
       &= \frac{1}{2} \Big( \mathbb{E} \big[ X^2 \big] - 2 \mathbb{E} \big[ X \widehat X^N \big] + \mathbb{E} \big[ \big(\widehat X^N \big)^2 \big]\Big)
    \end{aligned}
$$ -->

\[\label{eq:distortion}
	\mathcal{Q}_{2,N} : x \longmapsto \frac{1}{2} \mathbb{E} \Big[ \min_{i = 1, \dots, N} \vert X - x_i^N \vert^2 \Big] = \frac{1}{2} \Vert X - \widehat X^N \Vert_{_2}^2.\]

<p>Now, let us have a look at the gradient’s expression. If we know the density of $X$, then we can devise fast deterministic minimization procedures. Starting from the equation of the gradient of the distortion</p>

\[\nabla \mathcal{Q}_{2,N} (x) = \Big[ \mathbb{E}\big[ \mathbb{1}_{X \in C_i (\Gamma_N)} ( x_i^N - X ) \big] \Big]_{i = 1, \dots, N },\]

<p>it can be rewritten using the expression of the first partial moment and the cumulative distribution function of $X$, yielding</p>

\[\label{eq:grad_dist_deter}
    \nabla \mathcal{Q}_{2,N} (x) = \bigg[ x_i \Big( F_{_X} \big( x_{i+1/2}^N \big) - F_{_X} \big( x_{i-1/2}^N \big) \Big) - \Big( K_{_X} \big( x_{i+1/2}^N \big) - K_{_X} \big( x_{i-1/2}^N \big) \Big) \bigg]_{i = 1, \dots, N }\]

<p>where \(K_{_X}(\cdot)\) and \(F_{_X}(\cdot)\) are, respectively, the first partial moment and the cumulative distribution function of $X$</p>

\[K_{_X}(x) := \mathbb{E} [ X \mathbb{1}_{X \leq x} ] \qquad \textrm{ and } \qquad F_{_X}(x) := \mathbb{P} ( X \leq x ).\]

<p>Compared to the $\mathbb{R}^d$ case, where we needed to draw random samples of $X$ in order to approximate $\nabla \mathcal{Q}_{2,N} (x)$, now, if implementations of \(K_{_X}(\cdot)\) and \(F_{_X}(\cdot)\) exist, then the gradient will be computed very efficiently and the optimal quantization minimization problem will be speed up.</p>

<p>Hence, the following methods can be added to the abstract class <code class="language-plaintext highlighter-rouge">VoronoiQuantization1D</code> where</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">gradient_distortion</code> computes the quadratic distortion’s gradient for a given quantizer where the list of centroids <code class="language-plaintext highlighter-rouge">centroids</code> is <strong>sorted</strong>,</li>
  <li><code class="language-plaintext highlighter-rouge">cells_expectation</code> computes the expectation of $X$ on each cell using the first partial moment function,</li>
  <li><code class="language-plaintext highlighter-rouge">cells_probability</code> computes the probabilities of $X$ on each cell using the cumulative distribution function (that are the
      probabilities of the quantizer $\widehat X^N$),</li>
  <li><code class="language-plaintext highlighter-rouge">cdf</code> is an abstract method that needs to be implemented in the derived classes, it is the Cumulative Distribution Function \(F_{_X}(\cdot)\),</li>
  <li><code class="language-plaintext highlighter-rouge">fpm</code> is an abstract method that needs to be implemented in the derived classes, it is the First Partial Moment \(K_{_X}(\cdot)\).</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">VoronoiQuantization1D</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>

    <span class="p">...</span>

    <span class="k">def</span> <span class="nf">gradient_distortion</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">centroids</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="n">vertices</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">get_vertices</span><span class="p">(</span><span class="n">centroids</span><span class="p">)</span>
        <span class="n">to_return</span> <span class="o">=</span> <span class="n">centroids</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">cells_probability</span><span class="p">(</span><span class="n">vertices</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">cells_expectation</span><span class="p">(</span><span class="n">vertices</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">to_return</span>

    <span class="k">def</span> <span class="nf">cells_expectation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vertices</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="n">first_partial_moment</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fpm</span><span class="p">(</span><span class="n">vertices</span><span class="p">)</span>
        <span class="n">mean_on_each_cell</span> <span class="o">=</span> <span class="n">first_partial_moment</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">first_partial_moment</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">mean_on_each_cell</span>

    <span class="k">def</span> <span class="nf">cells_probability</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vertices</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="n">cumulated_probability</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">vertices</span><span class="p">)</span>
        <span class="n">proba_of_each_cell</span> <span class="o">=</span> <span class="n">cumulated_probability</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">cumulated_probability</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">proba_of_each_cell</span>

    <span class="o">@</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">cdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">]:</span>
        <span class="k">pass</span>

    <span class="o">@</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">fpm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">]:</span>
        <span class="k">pass</span>
</code></pre></div></div>

<h3 class="no_toc" id="remark">Remark</h3>

<p>The distortion can also be rewritten using \(K_{_X}(\cdot)\) and \(F_{_X}(\cdot)\)</p>

\[\begin{aligned}
	   \mathcal{Q}_{2,N} (x)
       &amp;= \frac{1}{2} \Big[ \mathbb{V}\textrm{ar} (X) + \mathbb{E} [ X ]^2 + \sum_{i=1}^N \big( x_i^N \big)^2 \Big( F_{_X} \big( x_{i+1/2}^N \big) - F_{_X} \big( x_{i-1/2}^N \big) \Big) \\
       &amp; \qquad  - 2 \sum_{i=1}^N x_i^N \Big( K_{_X} \big( x_{i+1/2}^N \big) - K_{_X} \big( x_{i-1/2}^N \big) \Big) \Big].
    \end{aligned}\]

<p>Using this new formula, we can easily compute the distortion using the methods already implemented in <code class="language-plaintext highlighter-rouge">VoronoiQuantization1D</code> as long as we know the mean and variance of $X$.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">VoronoiQuantization1D</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>

    <span class="p">...</span>

    <span class="n">mean</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">variance</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">distortion</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">centroids</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="n">vertices</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">get_vertices</span><span class="p">(</span><span class="n">centroids</span><span class="p">)</span>

        <span class="c1"># First term is variance of random variable
</span>        <span class="n">to_return</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">variance</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">mean</span> <span class="o">**</span> <span class="mi">2</span>

        <span class="c1"># Second term is 2 * \sum_i x_i * E [ X \1_{X \in C_i} ]
</span>        <span class="n">mean_of_each_cell</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cells_expectation</span><span class="p">(</span><span class="n">vertices</span><span class="p">)</span>
        <span class="n">to_return</span> <span class="o">-=</span> <span class="mf">2.</span> <span class="o">*</span> <span class="p">(</span><span class="n">centroids</span> <span class="o">*</span> <span class="n">mean_of_each_cell</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>

        <span class="c1"># Third and last term is E [ \widehat X^2 ]
</span>        <span class="n">proba_of_each_cell</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cells_probability</span><span class="p">(</span><span class="n">vertices</span><span class="p">)</span>
        <span class="n">to_return</span> <span class="o">+=</span> <span class="p">(</span><span class="n">centroids</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">proba_of_each_cell</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>

        <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">to_return</span>
</code></pre></div></div>

<p>In the one dimensional case, we have access to a closed-form formula (or efficient numerical implementation) of the density function, the cumulative distribution function and partial first moment for a lot of random variables. I detail below, for several random variables $X$, \(K_{_X}(\cdot)\), \(F_{_X}(\cdot)\) and \(\varphi_{_X}(\cdot)\), the first partial moment, the cumulative distribution function and the density of $X$, respectively. More formulas can be find in <a class="citation" href="#montes2020numerical">(Montes, 2020)</a> (e.g for Gamma distribution, Non-central $\chi^2(1)$ distribution, Supremum of the Brownian bridge, Symmetric random variable and more). Each example of distribution below is accompanied by its python implementation of the inherited class. The common imports for all classes are given below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">scipy</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="kn">from</span> <span class="nn">cmath</span> <span class="kn">import</span> <span class="n">inf</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Union</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">field</span>

<span class="kn">from</span> <span class="nn">univariate.voronoi_quantization</span> <span class="kn">import</span> <span class="n">VoronoiQuantization1D</span>
</code></pre></div></div>

<p><span style="color:#66CCFF">
    Click on the bold texts below to expand and see the formulas and details ⤵️
</span></p>

<p><details open="">
    <summary>
    <span style="font-weight:bold">
        Normal distribution
    </span>
    : $X \sim \mathcal{N} (\mu, \sigma )$ with $\mu \in \mathbb{R}$ and $\sigma &gt;0$
</summary>

    <p>If one needs to build an optimal quantizer $\widehat X_{\mu,\sigma}$ of a normal $X_{\mu,\sigma} \sim \mathcal{N} (\mu, \sigma)$ with $\mu \neq 0$ and/or $\sigma \neq 1$, then building an optimal quantizer of $\widehat X_{0, 1}$ of $X_{0, 1}$, we have $\widehat X_{\mu,\sigma} = \mu + \sigma \widehat X_{0, 1}$ because in dimension 1, linear transformations of a quantizer preserve its optimality.</p>

    <p>Let $\xi \in \mathbb{R}$,</p>

\[\varphi_{_X}(\xi) = \frac{\textrm{e}^{-\xi^2/2}}{\sqrt{2 \pi}} ,\qquad F_{_X}(\xi) = \mathcal{N}(\xi) ,\qquad K_{_X}(\xi) = -\varphi_{_X}(\xi).\]

    <p>I give below an implementation of the class <code class="language-plaintext highlighter-rouge">NormalVoronoiQuantization</code> inherited from <code class="language-plaintext highlighter-rouge">VoronoiQuantization1D</code> that implements the methods <code class="language-plaintext highlighter-rouge">cdf</code>, <code class="language-plaintext highlighter-rouge">pdf</code> and <code class="language-plaintext highlighter-rouge">fpm</code> for the case $\mu=0$ and $\sigma=1$.</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>


<span class="o">@</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">NormalVoronoiQuantization</span><span class="p">(</span><span class="n">VoronoiQuantization1D</span><span class="p">):</span>
    <span class="n">lower_bound_support</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">default</span><span class="o">=-</span><span class="n">inf</span><span class="p">)</span>
    <span class="n">upper_bound_support</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">inf</span><span class="p">)</span>
    <span class="n">mean</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">variance</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Probabilty Density Function
</span>    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">]):</span>
        <span class="k">return</span> <span class="n">norm</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Cumulative Distribution Function
</span>    <span class="k">def</span> <span class="nf">cdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">]):</span>
        <span class="k">return</span> <span class="n">norm</span><span class="p">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># First Partial Moment
</span>    <span class="k">def</span> <span class="nf">fpm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">]):</span>
        <span class="k">return</span> <span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">lr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">N</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">N</span> <span class="o">*</span> <span class="n">N</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">a</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mf">1.</span><span class="p">))</span>
</code></pre></div>    </div>

    <!-- <script src="https://gist.github.com/montest/58b06cc2e9659c7757981b384524d529.js"></script> -->
  </details></p>

<p><details>
    <summary>
    <span style="font-weight:bold">
        Log-normal distribution
    </span>
    : $X = \exp( \mu + \sigma Z)$ with $\mu \in \mathbb{R}$ and $\sigma &gt;0$ where $Z \sim \mathcal{N}(0,1)$
</summary>

    <p>If one needs to build an optimal quantizer $\widehat X_{\mu,\sigma}$ of a log-normal $X_{\mu,\sigma}=\exp( \mu + \sigma Z)$ with $\mu \neq 0$, then using the optimal quantizer $\widehat X_{0, \sigma}$ of $X_{0, \sigma}$, we have  $\widehat X_{\mu,\sigma} = \textrm{e}^{\mu} \widehat X_{0, \sigma}$ (using that same argument than in the normal case above).</p>

    <p>Let $\xi \in \mathbb{R}^{+*}$,</p>

\[\begin{aligned}
		      \varphi_{_X}(\xi)  &amp; = \frac{1}{\xi \sigma} \varphi_{_Z} \Big( \frac{\log(\xi)-\mu}{\sigma} \Big) ,\qquad F_{_X}(\xi) = \mathcal{N} \Big( \frac{\log(\xi)-\mu}{\sigma} \Big) , \\
		      \qquad K_{_X}(\xi) &amp; = \textrm{e}^{\mu + \sigma^2/2} \mathcal{N} \Big( \frac{\log(\xi)-\mu - \sigma^2}{\sigma} \Big)
    \end{aligned}\]

    <p>with $\varphi_{_Z}$ the density of $Z$.</p>

    <p>I give below an implementation of the class <code class="language-plaintext highlighter-rouge">LogNormalVoronoiQuantization</code> inherited from <code class="language-plaintext highlighter-rouge">VoronoiQuantization1D</code> that implements the methods <code class="language-plaintext highlighter-rouge">cdf</code>, <code class="language-plaintext highlighter-rouge">pdf</code> and <code class="language-plaintext highlighter-rouge">fpm</code> for the case $\mu=0$.</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">lognorm</span><span class="p">,</span> <span class="n">norm</span>


<span class="o">@</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">LogNormalVoronoiQuantization</span><span class="p">(</span><span class="n">VoronoiQuantization1D</span><span class="p">):</span>
    <span class="n">sigma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">lower_bound_support</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">upper_bound_support</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">inf</span><span class="p">)</span>
    <span class="n">mean</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">variance</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">variance</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">-</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Probabilty Density Function
</span>    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">]):</span>
        <span class="k">return</span> <span class="n">lognorm</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">sigma</span><span class="p">)</span>

    <span class="c1"># Cumulative Distribution Function
</span>    <span class="k">def</span> <span class="nf">cdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">]):</span>
        <span class="k">return</span> <span class="n">lognorm</span><span class="p">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">sigma</span><span class="p">)</span>

    <span class="c1"># First Partial Moment
</span>    <span class="k">def</span> <span class="nf">fpm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">]):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">mean</span> <span class="o">*</span> <span class="n">norm</span><span class="p">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">sigma</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">sigma</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">sigma</span><span class="p">)</span>
</code></pre></div>    </div>

    <!-- <script src="https://gist.github.com/montest/36b1db9f51474e1a1037385bee9b4cd9.js"></script> -->
  </details></p>

<p><details>
    <summary>
    <span style="font-weight:bold">
        Uniform distribution
    </span>
    : $X \sim \mathcal{U} (0,1)$
</summary>
    <p>Let $\xi \in [0,1]$,</p>

\[\varphi_{_X}(\xi) = 1 ,\qquad F_{_X}(\xi) = \xi ,\qquad K_{_X}(\xi) = \frac{\xi^2}{2}.\]

    <p>I give below an implementation of the class <code class="language-plaintext highlighter-rouge">UniformVoronoiQuantization</code> inherited from <code class="language-plaintext highlighter-rouge">VoronoiQuantization1D</code> that implements the methods <code class="language-plaintext highlighter-rouge">cdf</code>, <code class="language-plaintext highlighter-rouge">pdf</code>, <code class="language-plaintext highlighter-rouge">fpm</code> and <code class="language-plaintext highlighter-rouge">optimal_quantization</code> that return the optimal quantizer for a given size <code class="language-plaintext highlighter-rouge">N</code> using the closed form formula.</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">uniform</span>


<span class="o">@</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">UniformVoronoiQuantization</span><span class="p">(</span><span class="n">VoronoiQuantization1D</span><span class="p">):</span>
    <span class="n">lower_bound_support</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="n">upper_bound_support</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
    <span class="n">mean</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">1.0</span><span class="o">/</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="n">variance</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">1.0</span><span class="o">/</span><span class="mf">12.0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">optimal_quantization</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">centroids</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">((</span><span class="mf">2.0</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">N</span><span class="p">),</span> <span class="p">(</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">N</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">N</span><span class="p">),</span> <span class="n">N</span><span class="p">)</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cells_probability</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">get_vertices</span><span class="p">(</span><span class="n">centroids</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">centroids</span><span class="p">,</span> <span class="n">probabilities</span>

    <span class="c1"># Probabilty Density Function
</span>    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">]):</span>
        <span class="k">return</span> <span class="n">uniform</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Cumulative Distribution Function
</span>    <span class="k">def</span> <span class="nf">cdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">]):</span>
        <span class="k">return</span> <span class="n">uniform</span><span class="p">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># First Partial Moment
</span>    <span class="k">def</span> <span class="nf">fpm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">]):</span>
        <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span>
</code></pre></div>    </div>
    <!-- <script src="https://gist.github.com/montest/42367d0be4e882ec1596a38402f59cd0.js"></script> -->
  </details></p>

<p><details>
    <summary>
    <span style="font-weight:bold">
        Exponential distribution
    </span>
    : $X \sim \mathcal{E}(\lambda)$ with $\lambda &gt; 0$
</summary>
    <p>Let $\xi \in \mathbb{R}^{+}$,</p>

\[\varphi_{_X}(\xi) = \lambda \textrm{e}^{- \lambda \xi} ,\qquad F_{_X}(\xi) = 1 - \textrm{e}^{-\lambda \xi} ,\qquad K_{_X}(\xi) = - \textrm{e}^{- \lambda \xi} \Big( \xi + \frac{1}{\lambda} \Big) + \frac{1}{\lambda}.\]

    <p>I give below an implementation of the class <code class="language-plaintext highlighter-rouge">ExponentialVoronoiQuantization</code> inherited from <code class="language-plaintext highlighter-rouge">VoronoiQuantization1D</code> that implements the methods <code class="language-plaintext highlighter-rouge">cdf</code>, <code class="language-plaintext highlighter-rouge">pdf</code> and <code class="language-plaintext highlighter-rouge">fpm</code>.</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">univariate.voronoi_quantization</span> <span class="kn">import</span> <span class="n">VoronoiQuantization1D</span>


<span class="o">@</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">ExponentialVoronoiQuantization</span><span class="p">(</span><span class="n">VoronoiQuantization1D</span><span class="p">):</span>
    <span class="n">lambda_</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">lower_bound_support</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="n">upper_bound_support</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">inf</span><span class="p">)</span>
    <span class="n">mean</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">variance</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mean</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="bp">self</span><span class="p">.</span><span class="n">lambda_</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">variance</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">lambda_</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Probabilty Density Function
</span>    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">]):</span>
        <span class="k">return</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">lambda_</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Cumulative Distribution Function
</span>    <span class="k">def</span> <span class="nf">cdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">]):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">lambda_</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">lambda_</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># First Partial Moment
</span>    <span class="k">def</span> <span class="nf">fpm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">]):</span>
        <span class="n">to_return</span> <span class="o">=</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">lambda_</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">mean</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">mean</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="nb">float</span> <span class="ow">and</span> <span class="n">x</span> <span class="o">==</span> <span class="n">inf</span><span class="p">:</span>
            <span class="n">to_return</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mean</span>
        <span class="c1"># If x is an array then it is supposed to be sorted hence if there is an inf value, it is the last value and it
</span>        <span class="c1"># should appear only once in the case of optimal quantization.
</span>        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">inf</span><span class="p">:</span>
            <span class="n">to_return</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mean</span>
        <span class="k">return</span> <span class="n">to_return</span>
</code></pre></div>    </div>

    <!-- <script src="https://gist.github.com/montest/10d3ef7d9fa05aa247c1e38c2b624b17.js"></script> -->
  </details></p>

<!--
<p><details>
    <summary>
        <span style="font-weight:bold">
            Gamma distribution
        </span>
        : $X \sim \Gamma(\alpha, \beta)$ with $\alpha, \beta > 0$
    </summary>
    $$
		\varphi_{_X}(\xi) = \frac{\beta^\alpha}{\Gamma(\alpha)} \xi^{\alpha-1} \textrm{e}^{-\beta \xi} ,\qquad F_{_X}(\xi) = \frac{\gamma(\alpha, \beta \xi)}{\Gamma(\alpha)},\qquad K_{_X}(\xi) = F_{_X}(\xi) - \frac{\xi}{\beta} \varphi_{_X}(\xi),
    $$

	where $\Gamma(\cdot)$ is the gamma function and $$\gamma(s,x) = \int_{0}^{x} t^{s-1} e^{-t} dt$$ is the lower incomplete gamma function. Optimized numerical implementations for both functions can easily be find in any programming language.

</details></p>


<p><details>
    <summary>
        <span style="font-weight:bold">
            Non-central $\chi^2(1)$ distribution
        </span>
        : $X \sim \chi^2(1) = (Z + m)^2$ with $m \in \mathbb{R}$ where $Z \sim \mathcal{N}(0,1)$
    </summary>
    $$
		\begin{aligned}
			\varphi_{_X}(\xi) & = \frac{ \varphi_{_Z} \big( m+\sqrt{\xi} \big) + \varphi_{_Z} \big( m-\sqrt{\xi} \big) }{2 \sqrt{\xi}} ,\qquad F_{_X}(\xi) = \mathcal{N} \big( m+\sqrt{\xi} \big) - \mathcal{N} \big( m-\sqrt{\xi} \big) , \\
			K_{_X}(\xi)       & = \big( m-\sqrt{\xi} \big) \mathcal{N} \big( m+\sqrt{\xi} \big) - \big( m+\sqrt{\xi} \big) \mathcal{N} \big( m-\sqrt{\xi} \big) + \big( 1+m^2 \big) F_{_X} ( \xi ).
		\end{aligned}
    $$
</details></p>


<p><details>
    <summary>
        <span style="font-weight:bold">
            Supremum of the Brownian bridge
        </span>
        : $X = \textrm{sup}_{t \in [0, 1]} \vert W_t - t W_1 \vert$. This distribution is also known as the Kolmogorov-Smirnov distribution.
    </summary>
    $$
		\begin{aligned}
			\varphi_{_X}(\xi) & = 8 \xi \sum_{k \geq 1} (-1)^{k-1} k^2 \textrm{e}^{-2 k^2 \xi^2} , \qquad F_{_X}(\xi) = 1 - 2 \sum_{k \geq 1} (-1)^{k-1} \textrm{e}^{-2 k^2 \xi^2}, \\
	        K_{_X}(\xi)       & = \sqrt{2 \pi} \sum_{k \geq 1} \frac{ (-1)^{k-1} }{k} \Big( \mathcal{N} ( 2 k \xi ) - \frac{1}{2} \Big) - \xi (1 - F_{_X}(\xi)),
	    \end{aligned}
    $$

	where $\mathcal{N}(x)$ denotes the cumulative distribution function of the normal distribution. The proof of the formulas above are given in the Appendix of my PhD manuscript <a class="citation" href="#montes2020numerical">(Montes, 2020)</a>.
</details></p>

<p><details>
    <summary>
        <span style="font-weight:bold">
            Symmetric random variable
        </span>
        For some random variables $X$, we have no access to closed-form formulas for $\varphi_{_X}$, $F_{_X}$ and $K_{_X}$ but if $X$ is symmetric and we have an explicit expression for its characteristic function $\chi(u) = \mathbb{E} \big[ \textrm{e}^{\textbf{i} u X} \big]$, where $\textbf{i}$ is the imaginary number, s.t. $\textbf{i}^2=-1$, then the functions $\varphi_{_X}$, $F_{_X}$ and $K_{_X}$ can be written as alternate series using Fourier transform. This method was introduced in chapter $5$ of <a class="citation" href="#pages2018numerical">(Pagès, 2018)</a>. The proof of the formulas below are given in the Appendix of my PhD manuscript <a class="citation" href="#montes2020numerical">(Montes, 2020)</a>.
    </summary>
    For $\xi \geq 0$, we have

        $$
            \begin{aligned}
    		    \varphi_{_X}(\xi) & = \frac{1}{\pi \xi} \sum_{k \geq 0} (-1)^k \int_{0}^{\pi} \cos(u) \chi \Big( \frac{u + k \pi}{\xi} \Big) du, \\
    		    F_{_X}(\xi)       & = \frac{1}{2} + \frac{1}{\pi} \sum_{k \geq 0} (-1)^k \int_{0}^{\pi} \frac{\sin (u)}{u+k\pi} \chi \Big( \frac{u + k \pi}{\xi} \Big) du, \\
    			K_{_X}(\xi)       & = - C + \xi \Big( F_{_X}(\xi) - \frac{1}{2} \Big) + \frac{\xi}{\pi} \sum_{k \geq 0} \int_{0}^{\pi} \frac{ 1 - (-1)^k \cos (u)}{(u+k\pi)^2} \chi \Big( \frac{u + k \pi}{\xi} \Big) du,
    		\end{aligned}
        $$

    	where $C = \mathbb{E} [X_+]$ and for $\xi < 0$

        $$
    		\varphi_{_X}(\xi) = \varphi_{_X}(-\xi), \qquad F_{_X}(\xi) = 1 - F_{_X}(- \xi), \qquad K_{_X}(\xi) = K_{_X}(-\xi).
        $$

        <p>
        <span style="font-weight:bold">
            Examples
        </span>
        </p>

        We give some examples of symmetric random variables where we can use the above formulas based on Fourier in order to obtain the functions $\varphi_{_X}$, $F_{_X}$ and $K_{_X}$.

        <p>
        - <span style="font-weight:bold"> One-sided Lévy's area </span>: $X \sim \int_0^1 W_s^1 d W_s^2$ where $(W^1, W^2)$ is a $2$-dimensional standard Brownian motion. The characteristic function of the Lévy's area is given by

            $$
                \chi (u) = \frac{1}{\sqrt{\cosh (u)}} \quad \mbox{and} \quad C= 0.24852267 \pm 2.033 \times 10^{-7},
            $$

            where $C$ has been computed using a Richardson-Romberg-multilevel estimator.
        </p>

        <p>
        - <span style="font-weight:bold"> Standard normal distribution </span>: $X \sim \mathcal{N} (0,1)$. Although we have explicit formulas for the desired functions, we can still use the above formulas based on alternating series to the Gaussian case in order to validate the methodology. For the normal distribution, we have

            $$
                \chi (u) = \textrm{e}^{-u^2/2} \quad \mbox{and} \quad C = \frac{1}{\sqrt{2 \pi}}.
            $$
        </p>
</details></p>

<p><details>
    <summary>
        <span style="font-weight:bold">
            Closed-form formula of the characteristic function
        </span>
        Another method, introduced in <a class="citation" href="#callegaro2019quantization">(Callegaro et al., 2019)</a> for the quantization of a positive diffusion $(S_t)_{t \in [0, T]}$ at time $T$, is based on Fourier inversion in order to determine a computable expression of the density and the cumulative distribution function.
    </summary>
    They use the fact that the conditional characteristic function of $X = \log (S_T)$ is explicitly known or can be computed efficiently and denoted

        $$
    		\chi(u) = \mathbb{E} \big[ \textrm{e}^{\textbf{i} u \log (S_T)} \big], \qquad u \in \mathbb{R}.
        $$

    	Using the knowledge of the characteristic function of $X$, they obtain

        $$
    		\begin{aligned}
    			\mathbb{P} ( S_T \in dz ) & = \bigg( \frac{1}{\pi} \frac{1}{z} \int_0^{+ \infty} \textrm{Re} \big( \textrm{e}^{- \textbf{i} \log (z) \xi } \chi(u) \big) du \bigg) dz                                    \\
    			\mathbb{P} ( S_T \leq z ) & = \frac{1}{2} - \frac{1}{\pi} \int_0^{+ \infty} \textrm{Re} \bigg( \frac{\textrm{e}^{- \textbf{i} u \log (z) } \chi(u)}{\textbf{i} u} \bigg) du, \qquad z \in (0, + \infty).
            \end{aligned}
        $$
</details></p> -->

<h1 id="going-back-to-optimization-methods">Going back to optimization methods</h1>

<h2 id="fixed-point-search-deterministic-lloyd-method">Fixed-point search: Deterministic Lloyd method</h2>

<p>In my <a href="/2022/02/13/StochasticMethodsForOptimQuantif/">previous blog post</a>, I derived a fixed-point problem from Equation \ref{eq:grad_dist_deter}, such that</p>

\[\nabla \mathcal{Q}_{2,N} (x) = 0 \quad \iff \quad \forall i = 1, \dots, N  \qquad x_i = \Lambda_i ( x ).\]

<p>where</p>

\[\Lambda_i (x) = \frac{\mathbb{E}\big[ X \mathbb{1}_{ X \in C_i (\Gamma_N)} \big]}{\mathbb{P} \big( X \in C_i (\Gamma_N) \big)}\]

<p>which in the one-dimensional case can be rewritten using \(K_{_X}\) and \(F_{_X}\)</p>

\[\Lambda_i (x) = \frac{K_{_X} \big( x_{i+1/2}^N \big) - K_{_X} \big( x_{i-1/2}^N \big)}{F_{_X} \big( x_{i+1/2}^N \big) - F_{_X} \big( x_{i-1/2}^N \big)}.\]

<p>Hence, from this equality, we can write the following fixed-point search algorithm. This method is known as the deterministic Lloyd method. The Lloyd method was first devised by Lloyd in <a class="citation" href="#lloyd1982least">(Lloyd, 1982)</a>. Let \(\Lambda : \mathbb{R}^N \mapsto \mathbb{R}^N\) such that \(\Lambda = (\Lambda_i)_{1 \leq i \leq N}\), the Lloyd method with initial condition \(x^0\) is defined as follows</p>

\[x^{[n+1]} = \Lambda \big( x^{[n]} \big)\]

<p>where $x^{[n]}$ is the quantizer obtain after $n$ iterations of the algorithm. The pseudo-algorithm of the Lloyd method written on the vector \(x\) starting from a given quantizer $x^{0}$.</p>

<p>Using this formulation, we can easily implement a deterministic version of the Lloyd algorithm <code class="language-plaintext highlighter-rouge">deterministic_lloyd_method</code> using the methods already implemented in <code class="language-plaintext highlighter-rouge">VoronoiQuantization1D</code> that takes as input <code class="language-plaintext highlighter-rouge">centroids</code>, a list of centroids of size $N$ <strong>sorted</strong> by values (from lower to bigger) and <code class="language-plaintext highlighter-rouge">nbr_iterations</code>, the number of fixed point iterations we want to do.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">VoronoiQuantization1D</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>

    <span class="p">...</span>

    <span class="k">def</span> <span class="nf">deterministic_lloyd_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">centroids</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">nbr_iterations</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nbr_iterations</span><span class="p">):</span>
            <span class="n">vertices</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">get_vertices</span><span class="p">(</span><span class="n">centroids</span><span class="p">)</span>
            <span class="n">mean_of_each_cell</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cells_expectation</span><span class="p">(</span><span class="n">vertices</span><span class="p">)</span>
            <span class="n">proba_of_each_cell</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cells_probability</span><span class="p">(</span><span class="n">vertices</span><span class="p">)</span>
            <span class="n">centroids</span> <span class="o">=</span> <span class="n">mean_of_each_cell</span> <span class="o">/</span> <span class="n">proba_of_each_cell</span>

        <span class="n">probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cells_probability</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">get_vertices</span><span class="p">(</span><span class="n">centroids</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">centroids</span><span class="p">,</span> <span class="n">probabilities</span>
</code></pre></div></div>

<h3 class="no_toc" id="remark-1">Remark</h3>

<p>An interesting fact about the Lloyd method, is that each step of the fixed-point search preserve the sorting property of <code class="language-plaintext highlighter-rouge">centroids</code> (which is not always the case for the following algorithms).</p>

<h2 id="gradient-descent">Gradient descent</h2>

<p>Another approach for building an optimal quantizer consists in minimizing directly Equation \ref{eq:distortion} using a gradient descent. I detail two algorithms below: Mean-field CLVQ and Newton Raphson.</p>

<h3 id="mean-field-clvq">Mean-field CLVQ</h3>

<p>The first idea is to use a first-order gradient descent. This is the deterministic or batch version of the <strong>Competitive Learning Vector Quantization</strong> (CLVQ) algorithm detailed in my <a href="/2022/02/13/StochasticMethodsForOptimQuantif/">previous blog post</a>, which is a stochastic gradient descent introduced for the cases where we cannot numerically compute the gradient. In the one dimensional case, the gradient is easily computable using the expression of \(F_{_X}\) and \(K_{_X}\), as detailed in Equation \ref{eq:grad_dist_deter} hence a gradient descent is applied directly on the distortion. Starting from a given initial condition \(x^0\), the quantizer after \(n+1\) iterations is given by</p>

\[x^{[n+1]}  = x^{[n]} - \gamma_{n+1} \nabla \mathcal{Q}_{2,N} \big( x^{[n]} \big)\]

<p>where \(\gamma_{n+1} \in (0,1)\) is either taken constant (\(\gamma_{n+1} = \gamma\)) or updated at each step using a line search (see <a class="citation" href="#bonnans2006numerical">(Bonnans et al., 2006; Swann, 1969)</a>) or using the Barzilai–Borwein method (see <a class="citation" href="#barzilai1988two">(Barzilai &amp; Borwein, 1988)</a>).</p>

<p>I give below the Python implementation of the method <code class="language-plaintext highlighter-rouge">mean_field_clvq_method</code> that takes as input <code class="language-plaintext highlighter-rouge">centroids</code>, a list of <strong>sorted</strong> centroids and <code class="language-plaintext highlighter-rouge">nbr_iterations</code>, the number of gradient descend. By default, we chose <code class="language-plaintext highlighter-rouge">lr</code> to be equal to $0.1$ but more advanced learning rates can be chosen (eg. see the implementation of <code class="language-plaintext highlighter-rouge">lr</code> in the class <code class="language-plaintext highlighter-rouge">NormalVoronoiQuantization</code> where the origin of this function can be find in <a class="citation" href="#pages2018numerical">(Pagès, 2018)</a> or decreasing learning rates with a warm-up phase could also be considered).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">VoronoiQuantization1D</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>

    <span class="p">...</span>

    <span class="k">def</span> <span class="nf">mean_field_clvq_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">centroids</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">nbr_iterations</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nbr_iterations</span><span class="p">):</span>
            <span class="n">gradient</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">gradient_distortion</span><span class="p">(</span><span class="n">centroids</span><span class="p">)</span>
            <span class="n">lr</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lr</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">centroids</span><span class="p">),</span> <span class="n">i</span><span class="p">,</span> <span class="n">nbr_iterations</span><span class="p">)</span>
            <span class="n">centroids</span> <span class="o">=</span> <span class="n">centroids</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">gradient</span>
            <span class="n">centroids</span><span class="p">.</span><span class="n">sort</span><span class="p">()</span>

        <span class="n">probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cells_probability</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">get_vertices</span><span class="p">(</span><span class="n">centroids</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">centroids</span><span class="p">,</span> <span class="n">probabilities</span>

    <span class="k">def</span> <span class="nf">lr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">):</span>
        <span class="k">return</span> <span class="mf">0.1</span>
</code></pre></div></div>

<h3 id="newton-raphson-method">Newton-Raphson method</h3>

<p>One can optimize the algorithm defined above using a second-order method where the step \(\gamma_{n+1}\) is chosen optimally at each step and is set as the inverse of the Hessian matrix of the distortion function. This method is known as the <strong>Newton-Raphson method</strong> and was first use in the case of optimal quantization in <a class="citation" href="#pages2003optimal">(Pagès &amp; Printems, 2003)</a>. Again, starting from a initial condition \(x^0\) at step \(0\), we have</p>

\[x^{[n+1]} = x^{[n]} - \Big( \nabla^2 \mathcal{Q}_{2,N} \big( x^{[n]} \big) \Big)^{-1} \Big( \nabla \mathcal{Q}_{2,N} \big( x^{[n]} \big) \Big)\]

<p>with $\nabla^2 \mathcal{Q}_{2,N} (x)$ the Hessian matrix for $x = (x_1, \dots, x_N)$</p>

\[\nabla^2 \mathcal{Q}_{2,N} (x) = \bigg[ \frac{\partial^2 \mathcal{Q}_{2,N}}{\partial x_i \partial x_j} (x) \bigg]_{1 \leq i,j \leq N}.\]

<p>The Hessian matrix is tridiagonal and since we have access to \(X\)’s density and cumulative distribution functions, each component of the matrix can be computed efficiently with the following expression</p>

\[\begin{aligned}
		\frac{\partial^2 \mathcal{Q}_{2,N}}{\partial x_i^2} (x)                &amp; = 2 \big[ F_{_X}  \big( x_{i+1/2}^N \big) - F_{_X}  \big( x_{i-1/2}^N \big) \big] - \frac{x_{i+1}-x_i}{2} \varphi_{_X} \big( x_{i+1/2}^N \big) - \frac{x_{i}-x_{i-1}}{2} \varphi_{_X} \big( x_{i-1/2}^N \big), \\
		\frac{\partial^2 \mathcal{Q}_{2,N}}{\partial x_i \partial x_{i+1}} (x) &amp; = - \frac{x_{i+1}-x_i}{2} \varphi_{_X} \big( x_{i+1/2}^N \big),                                                                                                                                                \\
		\frac{\partial^2 \mathcal{Q}_{2,N}}{\partial x_i \partial x_{i-1}} (x) &amp; = - \frac{x_{i}-x_{i-1}}{2} \varphi_{_X} \big( x_{i-1/2}^N \big),                                                                                                                                              \\
		\frac{\partial^2 \mathcal{Q}_{2,N}}{\partial x_i \partial x_j} (x)     &amp; = 0 \textrm{ otherwise.}                                                                                                                                                                                       \\
	\end{aligned}\]

<p>The implementation of the method <code class="language-plaintext highlighter-rouge">hessian_distortion</code>, that for given list of centroids as input returns an array of size <code class="language-plaintext highlighter-rouge">(N, N)</code> containing the hessian matrix is given below. The abstract method <code class="language-plaintext highlighter-rouge">pdf</code> is defined in the inherited classes (eg, see the implementation of <code class="language-plaintext highlighter-rouge">NormalVoronoiQuantization</code>).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">VoronoiQuantization1D</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>

    <span class="p">...</span>

    <span class="k">def</span> <span class="nf">hessian_distortion</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">centroids</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">centroids</span><span class="p">)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>
        <span class="n">vertices</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">get_vertices</span><span class="p">(</span><span class="n">centroids</span><span class="p">)</span>
        <span class="n">proba_of_each_cell</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cells_probability</span><span class="p">(</span><span class="n">vertices</span><span class="p">)</span>
        <span class="n">tempDens</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">vertices</span><span class="p">)</span>

        <span class="n">a</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">proba_of_each_cell</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">tempDens</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">a</span>
            <span class="n">a</span> <span class="o">=</span> <span class="p">(</span><span class="n">centroids</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">centroids</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="mf">0.5</span>
            <span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">tempDens</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">a</span>
            <span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span> <span class="n">a</span> <span class="o">*</span> <span class="n">tempDens</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
            <span class="n">result</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">result</span><span class="p">[</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">proba_of_each_cell</span><span class="p">[</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">tempDens</span><span class="p">[</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">a</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="o">@</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">]:</span>
        <span class="k">pass</span>
</code></pre></div></div>

<p>The Python code of the Newton-Raphson method is detailed below, where in place of computing the inverse of the Hessian matrix, I solve the following linear system: I search for $u \in \mathbb{R}^N$ solution of</p>

\[\label{eq:linear_syst_hessian}
	Hu = G\]

<p>where \(H = \nabla^2 \mathcal{Q}_{2,N} (x)\) and \(G=\nabla \mathcal{Q}_{2,N} (x)\).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">VoronoiQuantization1D</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>

    <span class="p">...</span>

    <span class="k">def</span> <span class="nf">newton_raphson_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">centroids</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">nbr_iterations</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nbr_iterations</span><span class="p">):</span>
            <span class="n">hessian</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">hessian_distortion</span><span class="p">(</span><span class="n">centroids</span><span class="p">)</span>
            <span class="n">gradient</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">gradient_distortion</span><span class="p">(</span><span class="n">centroids</span><span class="p">)</span>
            <span class="n">inv_hessian_dot_grad</span> <span class="o">=</span> <span class="n">scipy</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">solve</span><span class="p">(</span><span class="n">hessian</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">assume_a</span><span class="o">=</span><span class="s">'sym'</span><span class="p">)</span>
            <span class="n">centroids</span> <span class="o">=</span> <span class="n">centroids</span> <span class="o">-</span> <span class="n">inv_hessian_dot_grad</span>
            <span class="n">centroids</span><span class="p">.</span><span class="n">sort</span><span class="p">()</span>  <span class="c1"># we sort the centroids because Newton-Raphson does not always preserve the order
</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cells_probability</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">get_vertices</span><span class="p">(</span><span class="n">centroids</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">centroids</span><span class="p">,</span> <span class="n">probabilities</span>
</code></pre></div></div>

<p>The problem with the Newton-Raphson algorithm is that it suffers from high instability when the Hessian is almost non-invertible. It often arises when we wish to build optimal quantizers for high values of $N$. Moreover, when $N$ is large, even though Newton-Raphson algorithm needs few iterations to converge (when it converges), each iteration requires a lengthy amount of time because of the space needed in memory in order to store the hessian matrix and solve the linear problem (find $u$ in Equation \ref{eq:linear_syst_hessian}) which results in slow convergence.</p>

<h1 id="numerical-examples">Numerical examples</h1>

<p>In this part, I test the previously detailed algorithms in order to build optimal quantizers for different distributions, namely: Normal, Log-Normal and Exponential. I plot the distortion as a function of the number of iteration of a given optimization method. <code class="language-plaintext highlighter-rouge">mfclvq</code> corresponds to the Mean-Field CLQV gradient descent, <code class="language-plaintext highlighter-rouge">lloyd</code> to the Lloyd method and <code class="language-plaintext highlighter-rouge">nr</code> to the Newton-Raphson gradient descent. For each distribution, I show the convergence for 2 sizes of grid: $N=10$ and $N=50$.</p>

<p>Several remarks can be made on the following figures:</p>

<ul>
  <li>First, one can notice that for $N=50$, I do not plot the results for the Newton-Raphson. It is because of the numerical instability induced by the “almost” non-invertible Hessian matrix. I will detail on future blog-posts (see also Chapter 1 in <a class="citation" href="#montes2020numerical">(Montes, 2020)</a>) how to fix this issue and still use the Hessian matrix when we wish to optimize optimal quantizers. However, in the case $N=10$, when the inverse of the Hessian is not ill-defined, we can notice that the Newton-Raphson algorithm converges very quickly and beats all the other methods.</li>
  <li>Second, in the normal distribution case, <code class="language-plaintext highlighter-rouge">mfclvq</code> converges very quickly even with higher values of $N$. This due to a well chosen learning rate (see <a class="citation" href="#pages2003optimal">(Pagès &amp; Printems, 2003)</a>) where the authors wish to taken into account the mode of the normal distribution. So, we can notice than the choice of the learning rate for the gradient descent is crucial and it can lead to really fast convergence as very slow (see for example the log normal distribution where we chose a constant learning rate equal to $0.1$).</li>
  <li>Finally, the Lloyd method gives consistent and very good results (it need less than a 100 iterations in order to reach the precision of the Newton-Raphson algorithm) for all 3 distributions and the 2 quantizer’s sizes making the Lloyd algorithm a very good and reliable method.</li>
</ul>

<h2 id="normal-distribution">Normal distribution</h2>

<center>
    <img alt="normal_10" src="/images/posts/quantization/univariate/normal_10.svg" width="600" />
    <img alt="normal_50" src="/images/posts/quantization/univariate/normal_50.svg" width="600" />
</center>

<h2 id="log-normal-distribution">Log-Normal distribution</h2>

<center>
    <img alt="lognormal_10" src="/images/posts/quantization/univariate/lognormal_10.svg" width="600" />
    <img alt="lognormal_50" src="/images/posts/quantization/univariate/lognormal_50.svg" width="600" />
</center>

<h2 id="exponential-distribution">Exponential distribution</h2>

<center>
    <img alt="exponential_10" src="/images/posts/quantization/univariate/exponential_10.svg" width="600" />
    <img alt="exponential_50" src="/images/posts/quantization/univariate/exponential_50.svg" width="600" />
</center>

<h1 id="references">References</h1>

<ol class="bibliography"><li><span id="montes2020numerical">Montes, T. (2020). <i>Numerical methods by optimal quantization in finance</i> [PhD thesis]. Sorbonne université.</span></li>
<li><span id="pages2018numerical">Pagès, G. (2018). <i>Numerical Probability: An Introduction with Applications to Finance</i>. Springer. https://doi.org/10.1007/978-3-319-90276-0</span></li>
<li><span id="callegaro2019quantization">Callegaro, G., Fiorin, L., &amp; Grasselli, M. (2019). Quantization meets Fourier: a new technology for pricing options. <i>Annals of Operations Research</i>, <i>282</i>(1-2), 59–86.</span></li>
<li><span id="lloyd1982least">Lloyd, S. (1982). Least squares quantization in PCM. <i>IEEE Transactions on Information Theory</i>, <i>28</i>(2), 129–137.</span></li>
<li><span id="bonnans2006numerical">Bonnans, J.-F., Gilbert, J. C., Lemaréchal, C., &amp; Sagastizábal, C. A. (2006). <i>Numerical optimization: theoretical and practical aspects</i>. Springer Science &amp; Business Media.</span></li>
<li><span id="swann1969survey">Swann, W. H. (1969). A survey of non-linear optimization techniques. <i>FEBS Letters</i>, <i>2</i>(S1), S39–S55.</span></li>
<li><span id="barzilai1988two">Barzilai, J., &amp; Borwein, J. M. (1988). Two-point step size gradient methods. <i>IMA Journal of Numerical Analysis</i>, <i>8</i>(1), 141–148.</span></li>
<li><span id="pages2003optimal">Pagès, G., &amp; Printems, J. (2003). Optimal quadratic quantization for numerics: the Gaussian case. <i>Monte Carlo Methods and Applications</i>, <i>9</i>(2), 135–165.</span></li></ol>

        
      </section>

      <footer class="page__meta">
        
        


  




  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="https://montest.github.io/tags/#numerical-probability" class="page__taxonomy-item" rel="tag">Numerical Probability</a><span class="sep">, </span>
    
      
      
      <a href="https://montest.github.io/tags/#optimal-quantization" class="page__taxonomy-item" rel="tag">Optimal Quantization</a><span class="sep">, </span>
    
      
      
      <a href="https://montest.github.io/tags/#optimization" class="page__taxonomy-item" rel="tag">Optimization</a>
    
    </span>
  </p>




      </footer>

      

<section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=https://montest.github.io/2022/06/21/DeterministicdMethodsForOptimQuantifUnivariates/" class="btn btn--twitter" title="Share on Twitter"><i class="fa fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https://montest.github.io/2022/06/21/DeterministicdMethodsForOptimQuantifUnivariates/" class="btn btn--facebook" title="Share on Facebook"><i class="fa fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://plus.google.com/share?url=https://montest.github.io/2022/06/21/DeterministicdMethodsForOptimQuantifUnivariates/" class="btn btn--google-plus" title="Share on Google Plus"><i class="fa fa-fw fa-google-plus" aria-hidden="true"></i><span> Google+</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://montest.github.io/2022/06/21/DeterministicdMethodsForOptimQuantifUnivariates/" class="btn btn--linkedin" title="Share on LinkedIn"><i class="fa fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>

      


  <nav class="pagination">
    
      <a href="https://montest.github.io/2022/02/13/StochasticMethodsForOptimQuantif/" class="pagination--pager" title="Stochastic Numerical Methods for Optimal Voronoï Quantization
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      
        <h4 class="page__related-title">You May Also Enjoy</h4>
      
      <div class="grid__wrapper">
        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://montest.github.io/2022/02/13/StochasticMethodsForOptimQuantif/" rel="permalink">Stochastic Numerical Methods for Optimal Voronoï Quantization
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  19 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2022-02-13T00:00:00+01:00">February 13, 2022</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p><img align="left" src="/images/posts/quantization/N_50_random_lloyd_100000.gif" width="150" /> In this post, I remind what is quadratic optimal quantizations. Then, I explain the two algorithms that were first devised in order to build an optimal quantization of a random vector $X$, namely: the fixed-point search called <strong>Lloyd method</strong> and the stochastic gradient descent known as <strong>Competitive Learning Vector Quantization</strong> (CLVQ). All explanations are accompanied by some code examples in Python and is available in the following Github repository: <a href="https://github.com/montest/stochastic-methods-optimal-quantization">montest/stochastic-methods-optimal-quantization</a>.</p>
</p>
    
    
    

  </article>
</div>

        
      </div>
    </div>
  

</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
      <li><a href="http://github.com/montest"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="https://montest.github.io/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 Thibaut Montes. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    <script src="https://montest.github.io/assets/js/main.min.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-5K8K92Y9P6"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-5K8K92Y9P6');
</script>








  </body>
</html>

